{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tabulate import tabulate\nimport os\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')\n%matplotlib inline\nimport copy\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, classification_report, confusion_matrix\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport glob\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.layers import Dropout\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_curve\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n       \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def print_df(df):\n    print(tabulate(df, headers=\"keys\",tablefmt=\"html\"))\n    \nRegularSeasonCompactResults = pd.read_csv('/kaggle/input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv')\n#print(\"size = \", RegularSeasonCompactResults.shape)\n#print_df(RegularSeasonCompactResults.head())\n\n\n\ndata_dict = {}\ndata_dict1 = {}\nfor i in glob.glob('/kaggle/input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/*'):\n    name = i.split('/')[-1].split('.')[0]\n    if name != 'MTeamSpellings':\n        data_dict[name] = pd.read_csv(i)\n    else:\n        data_dict[name] = pd.read_csv(i, encoding='cp1252')\n\nsub = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv')\n#sub.head()\n\n#data_dict.keys()\ndata_dict1 = data_dict.copy()\n\nfor name, df in data_dict.items():\n    print(f'{name}: {df.shape}')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display teams list\ndata_dict['MTeams'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Various names for each teams\ndata_dict['MTeamSpellings'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MNCAATourneySeeds'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert Alpha numeric seed to nummerical for comparision \ndata_dict['MNCAATourneySeeds']['Seed'] = data_dict['MNCAATourneySeeds']['Seed'].apply(lambda x: int(x[1:3]))\ndata_dict['MNCAATourneySeeds'] = data_dict['MNCAATourneySeeds'][['Season', 'TeamID', 'Seed']]\ndata_dict['MNCAATourneySeeds'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_temp =data_dict['MRegularSeasonCompactResults'].groupby('Season')[['WScore','LScore']].mean()\n\nfig = plt.gcf()\nfig.set_size_inches(28, 12)\nplt.plot(x_temp.index,x_temp['WScore'],marker='o', markerfacecolor='green', markersize=12, color='green', linewidth=4)\nplt.plot(x_temp.index,x_temp['LScore'],marker=7, markerfacecolor='red', markersize=12, color='red', linewidth=4)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_temp = pd.merge(data_dict['MNCAATourneyCompactResults'], data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\ndf_temp = pd.merge(df_temp, data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\nteam_win_score = data_dict['MRegularSeasonCompactResults'].groupby(['Season', 'WTeamID'])['WScore'].mean().reset_index()\n#print(team_win_score)\nteam_loss_score = data_dict['MRegularSeasonCompactResults'].groupby(['Season', 'LTeamID'])['LScore'].mean().reset_index()\ndf_temp = pd.merge(df_temp, team_win_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'WTeamID'])\ndf_temp = pd.merge(df_temp, team_loss_score, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'LTeamID'])\ndf_temp = pd.merge(df_temp, team_loss_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'LTeamID'])\ndf_temp = pd.merge(df_temp, team_win_score, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'WTeamID'])\ndf_temp.head()\n#data_dict1['MRegularSeasonCompactResults'].head()\n#df\n\ndf_temp['Score_Diff'] = df_temp['WScore_x'] - df_temp['WScore_y']\nplt.style.use('fivethirtyeight')\ndf_temp['Score_Diff'] \\\n    .plot(kind='hist',\n          bins=90,\n          figsize=(15, 5),\n          label='Mens',\n          alpha=0.5)\nplt.title('Score Differential')\nplt.xlim(0,60)\nplt.legend()\nplt.show()\n\ndata_dict['MNCAATourneyCompactResults'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mens regular wining results"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MRegularSeasonCompactResults']\ndata_dict['MRegularSeasonCompactResults'].groupby(['Season'])['WScore'].mean().plot();\nplt.title('Mean scores of winning teams by season in regular plays');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_dict['MNCAATourneyCompactResults'].groupby(['Season'])['WScore'].mean().plot(kind='line');\nplt.title('Mean scores of winning teams by season in tourneys');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MNCAATourneyCompactResults'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(data_dict['MNCAATourneyCompactResults'], data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\ndf = pd.merge(df, data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\ndf = df.drop(['TeamID_x', 'TeamID_y', 'DayNum','NumOT','WLoc','WScore','LScore'], axis=1)\ndf['seed_diff'] = df['Seed_x'] - df['Seed_y']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MNCAATourneyDetailedResults'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d = pd.merge(data_dict['MNCAATourneyCompactResults'], data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\ndf_d = pd.merge(df_d, data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\ndf_d = df_d.drop(['TeamID_x', 'TeamID_y', 'DayNum','NumOT','WLoc','WScore','LScore'], axis=1)\ndf_d['seed_diff'] = df_d['Seed_x'] - df_d['Seed_y']\ncols = ['FGM','FGA','FGM3','FGA3','FTM','FTA','OR','DR','Ast','TO','Stl','Blk','PF']\n# df_d['FGM_diff'] = df_d['WFGM'] - df_d['LFGM']\n# df_d['FGA_diff'] = df_d['WFGA'] - df_d['LFGA']\n# df_d['FGM3_diff'] = df_d['WFGM3'] - df_d['LFGM3']\n# df_d['FGA3_diff'] = df_d['WFGA3'] - df_d['LFGA3']\n# df_d['FTM_diff'] = df_d['WFTM'] - df_d['LFTM']\n# df_d['FTA_diff'] = df_d['WFTA'] - df_d['LFTA']\n# df_d['OR_diff'] = df_d['WOR'] - df_d['LOR']\n# df_d['DR_diff'] = df_d['WDR'] - df_d['LDR']\n# df_d['Ast_diff'] = df_d['WAst'] - df_d['LAst']\n# df_d['TO_diff'] = df_d['WTO'] - df_d['LTO']\n# df_d['Stl_diff'] = df_d['WStl'] - df_d['LStl']\n# df_d['Blk_diff'] = df_d['WBlk'] - df_d['LBlk']\n# df_d['PF_diff'] = df_d['WPF'] - df_d['LPF']\n#df_d = df_d.drop(['WFGM', 'WFGA','WFGM3','WFGA3','WFTM','WFTA','WOR','WDR','WAst','WTO','WStl','WBlk','WPF','LFGM','LFGA','LFGM3','LFGA3','LFTM','LFTA', 'LOR','LDR','LAst','LTO','LStl','LBlk','LPF'],axis=1)\n\ndf_d.head()\ndata_dict['MRegularSeasonCompactResults']['Season'] += 1\ndata_dict['MRegularSeasonDetailedResults']['Season'] += 1\n\nteam_win_score = data_dict['MRegularSeasonCompactResults'].groupby(['Season', 'WTeamID'])['WScore'].mean().reset_index()\n#print(team_win_score)\nteam_loss_score = data_dict['MRegularSeasonCompactResults'].groupby(['Season', 'LTeamID'])['LScore'].mean().reset_index()\n#team_loss_score\ndf_d = df_d.loc[(df_d['Season'] > 2004) & (df_d['Season'] < 2014)]\ndf_d = pd.merge(df_d, team_win_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'WTeamID'])\ndf_d = pd.merge(df_d, team_loss_score, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'LTeamID'])\ndf_d = pd.merge(df_d, team_loss_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'LTeamID'])\ndf_d = pd.merge(df_d, team_win_score, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'WTeamID'])\ndf_d.drop(['LTeamID_y', 'WTeamID_y'], axis=1, inplace=True)\n\n\nteam_win_FGM = data_dict['MRegularSeasonDetailedResults'].groupby(['Season', 'WTeamID'])['WFGM'].mean().reset_index()\n#print(team_win_score)\nteam_loss_FGM = data_dict['MRegularSeasonDetailedResults'].groupby(['Season', 'LTeamID'])['LFGM'].mean().reset_index()\n#print(team_loss_score)\ndf_d = pd.merge(df_d, team_win_FGM, how='left', left_on=['Season', 'WTeamID_x'], right_on=['Season', 'WTeamID'])\ndf_d = pd.merge(df_d, team_loss_FGM, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'LTeamID'])\ndf_d.drop(['LTeamID', 'WTeamID'], axis=1, inplace=True)\ndf_d = pd.merge(df_d, team_loss_FGM, how='left', left_on=['Season', 'WTeamID_x'], right_on=['Season', 'LTeamID'])\ndf_d = pd.merge(df_d, team_win_FGM, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'WTeamID'])\ndf_d.drop(['LTeamID', 'WTeamID'], axis=1, inplace=True)\n\n\n\n#df_d = df_d.loc[(df['Season'] > 2003) & (df['Season'] < 2014)]\ndf_d.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_dict['MRegularSeasonCompactResults']['Season'] += 1\n# data_dict['MRegularSeasonDetailedResults']['Season'] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# team_win_score = data_dict['MRegularSeasonCompactResults'].groupby(['Season', 'WTeamID'])['WScore'].mean().reset_index()\n# #print(team_win_score)\n# team_loss_score = data_dict['MRegularSeasonCompactResults'].groupby(['Season', 'LTeamID'])['LScore'].mean().reset_index()\n# #team_loss_score\n# df_d = df_d.loc[(df_d['Season'] > 2003) & (df_d['Season'] < 2014)]\n# df_d = pd.merge(df_d, team_win_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'WTeamID'])\n# df_d = pd.merge(df_d, team_loss_score, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'LTeamID'])\n# df_d = pd.merge(df_d, team_loss_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'LTeamID'])\n# df_d = pd.merge(df_d, team_win_score, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'WTeamID'])\n# df_d.drop(['LTeamID_y', 'WTeamID_y'], axis=1, inplace=True)\n\n\n# team_win_FGM = data_dict['MRegularSeasonDetailedResults'].groupby(['Season', 'WTeamID'])['WFGM'].mean().reset_index()\n# #print(team_win_score)\n# team_loss_FGM = data_dict['MRegularSeasonDetailedResults'].groupby(['Season', 'LTeamID'])['LFGM'].mean().reset_index()\n# #print(team_loss_score)\n# df_d = pd.merge(df_d, team_win_FGM, how='left', left_on=['Season', 'WTeamID_x'], right_on=['Season', 'WTeamID'])\n# df_d = pd.merge(df_d, team_loss_FGM, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'LTeamID'])\n# df_d.drop(['LTeamID', 'WTeamID'], axis=1, inplace=True)\n# df_d = pd.merge(df_d, team_loss_FGM, how='left', left_on=['Season', 'WTeamID_x'], right_on=['Season', 'LTeamID'])\n# df_d = pd.merge(df_d, team_win_FGM, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'WTeamID'])\n# df_d.drop(['LTeamID', 'WTeamID'], axis=1, inplace=True)\n\n\n\n# #df_d = df_d.loc[(df['Season'] > 2003) & (df['Season'] < 2014)]\n# df_d.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MRegularSeasonDetailedResults'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MRegularSeasonDetailedResults']['Season_join'] = data_dict['MRegularSeasonDetailedResults']['Season'] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WFGA'].mean().plot();\nplt.title('Average no of field goals Attempted vs Made by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WFGM'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average no of three pointers made by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WFGM3'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average no of three pointers attempted by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WFGA3'].mean().plot();\nplt.title('Average no of three pointers attempted vs made by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WFGM3'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WFTA'].mean().plot();\nplt.title('Average no of free throws attempted and made(green) by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WFTM'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average No of offensive rebounds pulled by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WOR'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average No of defensive rebounds pulled by winning teams pwe season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WDR'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average No of assists by winning teams by season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WAst'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average No of turnovers committed by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WTO'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average No of steals accomplished by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WStl'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Average No of blocks accomplished by winning teams per season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WBlk'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Mean number of personal fouls committed by winning teams by season');\ndata_dict['MRegularSeasonDetailedResults'].groupby(['Season'])['WPF'].mean().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_df = df_d[df_d['WTeamID_x'] > df_d['LTeamID_x']]\nwin_df = df_d[df_d['WTeamID_x'] < df_d['LTeamID_x']]\nwin_df['target'] = 1\n#win_df.columns = ['Season', 'Team1', 'Team2', 'Seed_1', 'Seed_2', 'seed_diff','FGM_diff','FGA_diff','FGM3_diff','FGA3_diff','FTM_diff','FTA_diff','OR_diff','DR_diff','Ast_diff','TO_diff','Stl_diff','Blk_diff','PF_diff','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'target']\nwin_df.columns = ['Season', 'Team1', 'Team2', 'Seed_1', 'Seed_2', 'seed_diff','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'WFGM_1','LFGM_1','LFGM_2','WFGM_2','target']\nloss_df['target'] = 0\nloss_df = loss_df[['Season', 'LTeamID_x', 'WTeamID_x', 'Seed_y', 'Seed_x', 'seed_diff','LScore_y', 'WScore_y', 'WScore_x', 'LScore_x', 'WFGM_x','LFGM_x','LFGM_y','WFGM_y','target']]\nloss_df.columns = ['Season', 'Team1', 'Team2', 'Seed_1', 'Seed_2', 'seed_diff','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2','WFGM_1','LFGM_1','LFGM_2','WFGM_2', 'target']\nloss_df['seed_diff'] = -1 * loss_df['seed_diff']\nnew_df = win_df.append(loss_df)\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv')\nsub.head()\ntest = sub.copy()\nsub['Season'] = sub['ID'].apply(lambda x: int(x.split('_')[0]))\nsub['Team1'] = sub['ID'].apply(lambda x: int(x.split('_')[1]))\nsub['Team2'] = sub['ID'].apply(lambda x: int(x.split('_')[2]))\n#sub = pd.merge(sub, data_dict['MRegularSeasonDetailedResults'], how='left', left_on=['Season', 'Team1', 'Team2'], right_on=['Season', 'WTeamID','LTeamID'])\nsub = pd.merge(sub, data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'Team1'], right_on=['Season', 'TeamID'])\nsub = pd.merge(sub, data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'Team2'], right_on=['Season', 'TeamID'])\nsub = pd.merge(sub, team_win_FGM, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'WTeamID'])\nsub = pd.merge(sub, team_loss_FGM, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'LTeamID'])\nsub.drop(['TeamID_x', 'TeamID_y','LTeamID','WTeamID'], axis=1, inplace=True)\nsub = pd.merge(sub, team_loss_FGM, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'LTeamID'])\nsub = pd.merge(sub, team_win_FGM, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'WTeamID'])\nsub.drop(['LTeamID', 'WTeamID'], axis=1, inplace=True)\nsub = pd.merge(sub, team_win_score, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'WTeamID'])\nsub = pd.merge(sub, team_loss_score, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'LTeamID'])\nsub = pd.merge(sub, team_loss_score, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'LTeamID'])\nsub = pd.merge(sub, team_win_score, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'WTeamID'])\nsub.drop(['WTeamID_x','LTeamID_x','LTeamID_y','WTeamID_y'], axis=1, inplace=True)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['seed_diff'] = sub['Seed_x'] - sub['Seed_y']\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare test data\n# sub = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv')\n# sub.head()\n# test = sub.copy()\n# sub['Season'] = sub['ID'].apply(lambda x: int(x.split('_')[0]))\n# sub['Team1'] = sub['ID'].apply(lambda x: int(x.split('_')[1]))\n# sub['Team2'] = sub['ID'].apply(lambda x: int(x.split('_')[2]))\n# sub = pd.merge(sub, data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'Team1'], right_on=['Season', 'TeamID'])\n# sub = pd.merge(sub, data_dict['MNCAATourneySeeds'], how='left', left_on=['Season', 'Team2'], right_on=['Season', 'TeamID'])\n# sub = pd.merge(sub, team_win_score, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'WTeamID'])\n# sub = pd.merge(sub, team_loss_score, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'LTeamID'])\n# sub = pd.merge(sub, team_loss_score, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'LTeamID'])\n# sub = pd.merge(sub, team_win_score, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'WTeamID'])\n# sub['seed_diff'] = sub['Seed_x'] - sub['Seed_y']\n# sub.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"team_counts = data_dict['MTeamSpellings'].groupby('TeamID')['TeamNameSpelling'].count().reset_index()\nteam_counts.columns = ['TeamID', 'TeamSpellingCount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = pd.merge(new_df, team_counts, how='left', left_on='Team1', right_on='TeamID')\nnew_df = new_df.drop(['TeamID'], axis=1)\nnew_df = pd.merge(new_df, team_counts, how='left', left_on='Team2', right_on='TeamID')\nnew_df = new_df.drop(['TeamID'], axis=1)\n\nsub = pd.merge(sub, team_counts, how='left', left_on='Team1', right_on='TeamID')\nsub = sub.drop(['TeamID'], axis=1)\nsub = pd.merge(sub, team_counts, how='left', left_on='Team2', right_on='TeamID')\nsub = sub.drop(['TeamID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = new_df.drop(['Season', 'Team1', 'Team2'], axis=1)\nsub = sub.drop(['Pred', 'Season', 'Team1', 'Team2'], axis=1)\nsub.columns = ['ID', 'Seed_1', 'Seed_2','WFGM_1','LFGM_1','LFGM_2','WFGM_2', 'WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'seed_diff', 'TeamSpellingCount_x', 'TeamSpellingCount_y']\nsub = sub[['ID', 'Seed_1', 'Seed_2', 'seed_diff', 'WFGM_1','LFGM_1','LFGM_2','WFGM_2','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'TeamSpellingCount_x', 'TeamSpellingCount_y']]\nnew_df = new_df.fillna(0)\nsub = sub.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_df.drop(['target'], axis=1)\ny = new_df['target']\nX_test = sub.drop(['ID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    y_pred_valid_global = []\n    y_valid_global = []\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y.values[train_index], y.values[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            y_pred_valid_global = y_pred_valid\n            y_valid_global = y_valid\n            print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid)[:, 1].reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            y_pred_valid_global = y_pred_valid\n            y_valid_global = y_valid\n            print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n        if model_type == 'sklearn1':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            y_pred_valid_global = y_pred_valid\n            y_valid_global = y_valid\n            print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.1, loss_function='Logloss',  eval_metric='AUC')\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)[:, 1].reshape(-1,)\n            #y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test)[:, 1]\n            y_pred_valid_global = y_pred_valid\n            y_valid_global = y_valid\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(log_loss(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance, y_pred_valid_global, y_valid_global\n        return oof, prediction, scores, y_pred_valid_global, y_valid_global\n    \n    else:\n        return oof, prediction, scores, y_pred_valid_global, y_valid_global","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = X.copy()\nX_test1 = X_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX1[['WFGM_1','LFGM_1','LFGM_2','WFGM_2','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']] = scaler.fit_transform(X1[['WFGM_1','LFGM_1','LFGM_2','WFGM_2','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']])\nX_test1[['WFGM_1','LFGM_1','LFGM_2','WFGM_2','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']] = scaler.transform(X_test1[['WFGM_1','LFGM_1','LFGM_2','WFGM_2','WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\nmodel = linear_model.LogisticRegression(C=0.0001)\noof_lr, prediction_logistic_reg, scores, y_pred_valid_global, y_valid_global = train_model(X1, X_test1, y, params=None, folds=folds, model_type='sklearn', model=model)\n\ntest['Pred'] = prediction_logistic_reg\ntest.to_csv('logistic_reg_new.csv', index=False)\n# print(y_pred_valid_global)\n# print(y_valid_global)\n# print(min(y_pred_valid_global))\nlog_reg_fpr, log_reg_ns_tpr, _ = roc_curve(y_valid_global, y_pred_valid_global)\n\n\n# show the legend\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noof_lr, prediction_cat_reg, scores, y_pred_valid_global, y_valid_global = train_model(X1, X_test1, y, params=None, folds=folds, model_type='cat')\n\ntest['Pred'] = prediction_cat_reg\ntest.to_csv('cat_new.csv', index=False)\ncat_reg_fpr, cat_reg_ns_tpr, _ = roc_curve(y_valid_global, y_pred_valid_global)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegression()\noof_lr, prediction_linear_r, scores, y_pred_valid_global, y_valid_global = train_model(X1, X_test1, y, params=None, folds=folds, model_type='sklearn1', model=model)\ntest['Pred'] = prediction_linear_r\ntest.to_csv('linear_reg.csv', index=False)\nlin_reg_fpr, lin_reg_ns_tpr, _ = roc_curve(y_valid_global, y_pred_valid_global)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AdaBoostRegressor( n_estimators=100,learning_rate=0.005, loss='exponential')\noof_lr, prediction_ada, scores,y_pred_valid_global, y_valid_global = train_model(X1, X_test1, y, params=None, folds=folds, model_type='sklearn1', model=model)\ntest['Pred'] = prediction_ada\ntest.to_csv('adaboost.csv', index=False)\nada_reg_fpr, ada_reg_ns_tpr, _ = roc_curve(y_valid_global, y_pred_valid_global)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsRegressor(n_neighbors=20)\noof_lr, prediction_knn, scores, y_pred_valid_global, y_valid_global = train_model(X1, X_test1, y, params=None, folds=folds, model_type='sklearn1', model=model)\ntest['Pred'] = prediction_knn\ntest.to_csv('knn.csv', index=False)\nknn_reg_fpr, knn_reg_ns_tpr, _ = roc_curve(y_valid_global, y_pred_valid_global)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers import BatchNormalization\n# def baseline_model():\n#     # create model\n#     model = Sequential()\n#     model.add(Dense(20, input_dim=9, kernel_initializer='normal', activation='relu'))\n#     model.add(BatchNormalization())\n#     model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n#     model.add(BatchNormalization())\n#     model.add(Dropout(0.5))\n# #model.add(BatchNormalization())\n#     model.add(Dense(1, kernel_initializer='normal'))\n#     # Compile model\n#     model.compile(loss='mean_squared_logarithmic_error', optimizer='adam')\n#     return model\n# model = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=20, verbose=0)\n# oof_lr, prediction_lr, scores = train_model(X1, X_test1, y, params=None, folds=folds, model_type='sklearn1', model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 8,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 5,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}\noof_lgb, prediction_lgb, scores, y_pred_valid_global, y_valid_global = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\nlgb_reg_fpr, lgb_reg_ns_tpr, _ = roc_curve(y_valid_global, y_pred_valid_global)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.plot(lgb_reg_fpr, lgb_reg_ns_tpr,'go', label='Light GBD',color='green')\npyplot.plot(knn_reg_fpr, knn_reg_ns_tpr, 'g^', label='KNN',color='b')\npyplot.plot(log_reg_fpr, log_reg_ns_tpr, 'g<', label='Logistic', color='r')\npyplot.plot(ada_reg_fpr, ada_reg_ns_tpr, 'g>', label='Adaboost', color='c')\npyplot.plot(lin_reg_fpr, lin_reg_ns_tpr, 'gv', label='LinearRegressor', color='m')\npyplot.plot(cat_reg_fpr, cat_reg_ns_tpr, 'g*', label='Catboost', color='y')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntest['Pred'] = prediction_lgb\ntest.to_csv('lgb.csv', index=False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}